{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7e9f53-4141-4f52-b704-49fb14e23518",
   "metadata": {},
   "source": [
    "# Speech Emotion Recognition using MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5d50b-86eb-4614-83a9-d1c83abc638a",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "This project presents a deep learning classifier able to predict the emotions of a human speaker encoded in an audio file. The classifier is trained using 2 different datasets, RAVDESS and TESS on 8 classes (neutral, calm, happy, sad, angry, fearful, disgust and surprised).\n",
    "\n",
    "#### Feature set information\n",
    "\n",
    "For this task, the dataset is built using 5252 samples from:\n",
    "\n",
    "- the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) dataset\n",
    "- the Toronto emotional speech set (TESS) dataset\n",
    "\n",
    "#### The samples include:\n",
    "\n",
    "- 1440 speech files and 1012 Song files from RAVDESS. This dataset includes recordings of 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech includes calm, happy, sad, angry, fearful, surprise, and disgust expressions, and song contains calm, happy, sad, angry, and fearful emotions. Each file was rated 10 times on emotional validity, intensity, and genuineness. Ratings were provided by 247 individuals who were characteristic of untrained adult research participants from North America. A further set of 72 participants provided test-retest data. High levels of emotional validity, interrater reliability, and test-retest intrarater reliability were reported. Validation data is open-access, and can be downloaded along with our paper from PLoS ONE.\n",
    "\n",
    "- 2800 files from TESS. A set of 200 target words were spoken in the carrier phrase \"Say the word _____' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 stimuli in total. Two actresses were recruited from the Toronto area. Both actresses speak English as their first language, are university educated, and have musical training. Audiometric testing indicated that both actresses have thresholds within the normal range.\n",
    "\n",
    "The classes the model wants to predict are the following: (0 = neutral, 1 = calm, 2 = happy, 3 = sad, 4 = angry, 5 = fearful, 6 = disgust, 7 = surprised)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd5210-63fd-4ff1-9940-2b62a0b8d70a",
   "metadata": {},
   "source": [
    "#### File naming convention\n",
    "\n",
    "Each of the 7356 RAVDESS files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 02-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
    "\n",
    "***Filename identifiers***\n",
    "\n",
    "- Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
    "- Vocal channel (01 = speech, 02 = song).\n",
    "- Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
    "- Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the ‘neutral’ emotion.\n",
    "- Statement (01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”).\n",
    "- Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
    "- Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
    "\n",
    "***Filename example: 02-01-06-01-02-01-12.wav***\n",
    "\n",
    "- Video-only (02)\n",
    "- Speech (01)\n",
    "- Fearful (06)\n",
    "- Normal intensity (01)\n",
    "- Statement “dogs” (02)\n",
    "- 1st Repetition (01)\n",
    "- 12th Actor (12)\n",
    "- Female, as the actor ID number is even."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe83c77d-c3d7-4e85-8839-96ab3af3ac06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Install all the Reqiuired Libraries and Packages \n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile \n",
    "from scipy.fftpack import fft\n",
    "from python_speech_features import mfcc , logfbank\n",
    "import librosa as lr\n",
    "import pickle\n",
    "import soundfile\n",
    "import speech_recognition as sr\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "get_ipython().magic('matplotlib inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4f9706b-ff9c-4d16-99c9-07237fd38f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting base directory for the project\n",
    "CURR_DIR = os.getcwd()\n",
    "# print(CURR_DIR)\n",
    "# BASE_DIR = f\"{CURR_DIR}/drive/MyDrive/project-final\"\n",
    "BASE_DIR = CURR_DIR\n",
    "# print(f\"{BASE_DIR}\")\n",
    "\n",
    "DATASET_PATH = os.path.join(BASE_DIR,'datasets')\n",
    "# print(DATASET_PATH)\n",
    "TRAINING_FILES_PATH = os.path.join(BASE_DIR,'training_dataset')\n",
    "# print(TRAINING_FILES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9d1b052-f345-4ed4-ae0d-fcabcde743a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Done ********************\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "We are filling folders Actor_25 if YAF and Actor_26 if OAF.\n",
    "The files will be copied and renamed and not simply moved (to avoid messing up\n",
    "things during the development of the pipeline.\n",
    "Actor_25 and Actor_26 folders must be created before launching this script.\n",
    "Example filename: 03-01-07-02-02-01-01.wav\n",
    "\"\"\"\n",
    "\n",
    "path = os.path.join(DATASET_PATH,'TESS_Toronto_emotional_speech_dataset')\n",
    "counter = 0\n",
    "\n",
    "label_conversion = {'01': 'neutral',\n",
    "                    '03': 'happy',\n",
    "                    '04': 'sad',\n",
    "                    '05': 'angry',\n",
    "                    '06': 'fear',\n",
    "                    '07': 'disgust',\n",
    "                    '08': 'ps'}\n",
    "'''\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for filename in files:\n",
    "        if filename.startswith('OAF'):\n",
    "            destination_path = os.path.join(TRAINING_FILES_PATH,'Actor_26')\n",
    "            # print(destination_path)\n",
    "            old_file_path = os.path.join(os.path.abspath(subdir), filename)\n",
    "            # print(old_file_path)\n",
    "\n",
    "            # Separate base from extension\n",
    "            base, extension = os.path.splitext(filename)\n",
    "\n",
    "            for key, value in label_conversion.items():\n",
    "                if base.endswith(value):\n",
    "                    random_list = random.sample(range(10, 99), 7)\n",
    "                    file_name = '-'.join([str(i) for i in random_list])\n",
    "                    file_name_with_correct_emotion = file_name[:6] + key + file_name[8:] + extension\n",
    "                    # new_file_path = destination_path + file_name_with_correct_emotion\n",
    "                    new_file_path = os.path.join(destination_path ,file_name_with_correct_emotion)\n",
    "                    # print(new_file_path)\n",
    "                    shutil.copy(old_file_path, new_file_path)\n",
    "\n",
    "        else:\n",
    "            destination_path = os.path.join(TRAINING_FILES_PATH,'Actor_25')\n",
    "            old_file_path = os.path.join(os.path.abspath(subdir), filename)\n",
    "\n",
    "            # Separate base from extension\n",
    "            base, extension = os.path.splitext(filename)\n",
    "\n",
    "            for key, value in label_conversion.items():\n",
    "                if base.endswith(value):\n",
    "                    random_list = random.sample(range(10, 99), 7)\n",
    "                    file_name = '-'.join([str(i) for i in random_list])\n",
    "                    file_name_with_correct_emotion = (file_name[:6] + key + file_name[8:] + extension).strip()\n",
    "                    new_file_path = os.path.join(destination_path ,file_name_with_correct_emotion)\n",
    "                    shutil.copy(old_file_path, new_file_path)\n",
    "'''\n",
    "print(\"********** Done ********************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65139364-16ff-404c-9b62-ddd12b8628c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now Cleaning Step is Performed where:\n",
    "#DOWN SAMPLING OF AUDIO FILES IS DONE  AND PUT MASK OVER IT AND DIRECT INTO CLEAN FOLDER\n",
    "#MASK IS TO REMOVE UNNECESSARY EMPTY VOIVES AROUND THE MAIN AUDIO VOICE \n",
    "def envelope(y , rate, threshold):\n",
    "    mask=[]\n",
    "    y=pd.Series(y).apply(np.abs)\n",
    "    y_mean = y.rolling(window=int(rate/10) ,  min_periods=1 , center = True).mean()\n",
    "    for mean in y_mean:\n",
    "        if mean>threshold:\n",
    "            mask.append(True)\n",
    "        else:\n",
    "            mask.append(False)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc284a18-fc52-4651-8c16-203567f27249",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5251/5251 [17:35<00:00,  4.97it/s]\n"
     ]
    }
   ],
   "source": [
    "#The clean Audio Files are redirected to Clean Audio Folder Directory \n",
    "\n",
    "for file in tqdm(glob.glob(f'{TRAINING_FILES_PATH}/**/*.wav')):\n",
    "    file_name = os.path.basename(file)\n",
    "    signal , rate = lr.load(file, sr=16000)\n",
    "    mask = envelope(signal,rate, 0.0005)\n",
    "    wavfile.write(filename= f'{BASE_DIR}/clean_speech/'+str(file_name), rate=rate,data=signal[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7384551-16bd-4e76-a8ea-a58a594ab663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotions in the RAVDESS dataset to be classified Audio Files based on . \n",
    "emotions={\n",
    "  '01':'neutral',\n",
    "  '02':'calm',\n",
    "  '03':'happy',\n",
    "  '04':'sad',\n",
    "  '05':'angry',\n",
    "  '06':'fearful',\n",
    "  '07':'disgust',\n",
    "  '08':'surprised'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a013c5fd-f6b9-42bc-90ad-34ffa076efa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features (mfcc, chroma, mel) from a sound file\n",
    "def extract_feature(file_name, mfcc, chroma, mel):\n",
    "    with soundfile.SoundFile(file_name) as sound_file:\n",
    "        X = sound_file.read(dtype=\"float32\")\n",
    "        sample_rate=sound_file.samplerate\n",
    "        if chroma:\n",
    "            stft=np.abs(lr.stft(X))\n",
    "        result=np.array([])\n",
    "        if mfcc:\n",
    "            mfccs=np.mean(lr.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
    "            result=np.hstack((result, mfccs))\n",
    "        if chroma:\n",
    "            chroma=np.mean(lr.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, chroma))\n",
    "        if mel:\n",
    "            mel=np.mean(lr.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
    "            result=np.hstack((result, mel))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d991b487-f250-46ce-a056-20a7ef3147d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#Load the data and extract features for each sound file\n",
    "def load_data(test_size=0.33):\n",
    "    x,y=[],[]\n",
    "    answer = 0\n",
    "    for file in glob.glob(f\"{BASE_DIR}/clean_speech/*.wav\"):\n",
    "        file_name=os.path.basename(file)\n",
    "        # print(file_name)\n",
    "        emotion=emotions[file_name.split(\"-\")[2]]\n",
    "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "        x.append(feature)\n",
    "        y.append([emotion,file_name])\n",
    "\n",
    "    return train_test_split(np.array(x), y, test_size=test_size, random_state=9)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3520863-3300-4a57-836a-a3e8e3053b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938, 180) (1313, 180) (3938, 2) (1313, 2)\n",
      "(3938,) (1313,)\n"
     ]
    }
   ],
   "source": [
    "x_train,x_test,y_train,y_test=load_data(test_size=0.25)\n",
    "print(np.shape(x_train),np.shape(x_test), np.shape(y_train),np.shape(y_test))\n",
    "y_test_map = np.array(y_test).T\n",
    "y_test = y_test_map[0]\n",
    "test_filename = y_test_map[1]\n",
    "y_train_map = np.array(y_train).T\n",
    "y_train = y_train_map[0]\n",
    "train_filename = y_train_map[1]\n",
    "print(np.shape(y_train),np.shape(y_test))\n",
    "# print(*test_filename,sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e02f7e8-7c46-4de5-bb73-271487951c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3938, 1313)\n",
      "Features extracted: 180\n"
     ]
    }
   ],
   "source": [
    "#Get the shape of the training and testing datasets\n",
    "print((x_train.shape[0], x_test.shape[0]))\n",
    "#Get the number of features extracted\n",
    "print(f'Features extracted: {x_train.shape[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac73ab86-c4e5-4fea-be04-56e70874799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the Multi Layer Perceptron Classifier\n",
    "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(400, 350, 300), learning_rate='adaptive', max_iter=2000,early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "837f0ebf-aa20-4ed6-a29b-e7412ca624b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "model = model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb27c055-9dee-4daf-98b2-c6f93cf1d1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Modle to file in the current working directory\n",
    "#For any new testing data other than the data in dataset\n",
    "model_filename = f\"{BASE_DIR}/model/SER_Model.pkl\"  \n",
    "\n",
    "with open(model_filename, 'wb') as file:  \n",
    "    pickle.dump(model, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eed63162-1d88-4db4-9d2c-b628dc9cc417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model back from file \n",
    "with open(model_filename, 'rb') as file:\n",
    "    SER_Model = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c3375694-0bf1-4095-9bf6-9d2815f88195",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting :\n",
    "# x_test=train_test_split(test_size=0.25)\n",
    "y_pred=SER_Model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5eef3b33-df08-41ae-a658-da5b50ba435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       angry       0.95      0.82      0.88       188\n",
      "        calm       0.84      0.84      0.84       104\n",
      "     disgust       0.78      0.93      0.84       149\n",
      "     fearful       0.72      0.88      0.79       185\n",
      "       happy       0.80      0.83      0.81       173\n",
      "     neutral       0.96      0.86      0.91       165\n",
      "         sad       0.90      0.76      0.82       201\n",
      "   surprised       0.85      0.86      0.86       148\n",
      "\n",
      "    accuracy                           0.84      1313\n",
      "   macro avg       0.85      0.84      0.84      1313\n",
      "weighted avg       0.85      0.84      0.84      1313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a6d71942-4a5c-4abd-a1c2-aa69046d3644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting loss curve\n",
    "plt.plot(model.loss_curve_)\n",
    "plt.title = \"Loss Curve\"\n",
    "plt.xlabel = 'Iterations'\n",
    "plt.ylabel = 'Cost'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a3e13886-c99a-49bc-8961-9f85c4ae728d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.14%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the accuracy of our model. \n",
    "accuracy = accuracy_score(y_true=y_test, y_pred=y_pred) \n",
    "# Print the accuracy \n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "57e7f288-d423-4951-a11c-793ab1fed006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     predictions                file_names\n",
      "0            sad  57-46-04-37-87-52-73.wav\n",
      "1      surprised  03-01-08-01-02-02-15.wav\n",
      "2        disgust  66-89-07-78-34-70-19.wav\n",
      "3      surprised  64-60-08-38-58-61-45.wav\n",
      "4        neutral  18-78-01-65-98-63-41.wav\n",
      "...          ...                       ...\n",
      "1308       angry  03-01-05-02-01-01-24.wav\n",
      "1309         sad  15-87-04-88-47-39-12.wav\n",
      "1310         sad  38-94-04-98-85-40-57.wav\n",
      "1311       happy  03-01-03-01-02-02-21.wav\n",
      "1312         sad  03-02-04-02-01-01-16.wav\n",
      "\n",
      "[1313 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = pd.DataFrame(y_pred, columns=['predictions'])\n",
    "y_pred1['file_names'] = test_filename\n",
    "print(y_pred1)\n",
    "y_pred1.to_csv(f'{BASE_DIR}/prediction.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f5b983ef-9277-4a79-8268-c4a0c8d4ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\project-main\\.venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:611: UserWarning: Got `batch_size` less than 1 or larger than sample size. It is going to be clipped\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_TRAIN_SAMPLES = x_train.shape[0]\n",
    "\n",
    "N_EPOCHS = 25\n",
    "N_BATCH = 128\n",
    "N_CLASSES = np.unique(y_train)\n",
    "\n",
    "scores_train = []\n",
    "scores_test = []\n",
    "\n",
    "# EPOCH\n",
    "epoch = 0\n",
    "while epoch < N_EPOCHS:\n",
    "    print('epoch: ', epoch)\n",
    "    # SHUFFLING\n",
    "    random_perm = np.random.permutation(x_train.shape[0])\n",
    "    mini_batch_index = 0\n",
    "    while True:\n",
    "        # MINI-BATCH\n",
    "        indices = random_perm[mini_batch_index:mini_batch_index + N_BATCH]\n",
    "        model.partial_fit(x_train[indices], y_train[indices], classes=N_CLASSES)\n",
    "        mini_batch_index += N_BATCH\n",
    "\n",
    "        if mini_batch_index >= N_TRAIN_SAMPLES:\n",
    "            break\n",
    "\n",
    "    # SCORE TRAIN\n",
    "    scores_train.append(model.score(x_train, y_train))\n",
    "\n",
    "    # SCORE TEST\n",
    "    scores_test.append(model.score(x_test, y_test))\n",
    "\n",
    "    epoch += 1\n",
    "\n",
    "\"\"\" Plot \"\"\"\n",
    "fig, ax = plt.subplots(2, sharex=True, sharey=True)\n",
    "ax[0].plot(scores_train)\n",
    "ax[0].set_title('Train')\n",
    "ax[1].plot(scores_test)\n",
    "ax[1].set_title('Test')\n",
    "fig.suptitle(\"Accuracy over epochs\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5ab186-0447-4b3c-95cf-d55397a89a3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c26b32ff-6e86-4982-ab7d-9a58d9b4eb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File : output10.wav  => Emotion : angry\n",
      "File : YAF_book_happy.wav  => Emotion : angry\n"
     ]
    }
   ],
   "source": [
    "# running examples files store in examples directory\n",
    "# to test new files upload it in examples directory\n",
    "for file in glob.glob(f'{BASE_DIR}/examples/*.*'):\n",
    "    data, sampling_rate = lr.load(file)\n",
    "    ## Appying extract_feature function on random file and then loading model to predict the result \n",
    "    ans =[]\n",
    "    new_feature = extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
    "    ans.append(new_feature)\n",
    "    ans = np.array(ans)\n",
    "    predicted = SER_Model.predict(ans)\n",
    "    print(f\"File : {os.path.basename(file)}  => Emotion : {predicted[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "621b52ba-e1f4-4ae8-acc1-f023c37b8d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "emotion = ['Angry',\n",
    "'Calm',\n",
    "'Disgust',\n",
    "'Fearful',\n",
    "'Happy',\n",
    "'neutral',\n",
    "'sad',\n",
    "'Surprised']\n",
    "accuracy = [0.93,\n",
    "            0.93,\n",
    "            0.85,\n",
    "            0.85,\n",
    "            0.89,\n",
    "            0.93,\n",
    "            0.83,\n",
    "            0.83]\n",
    "plt.plot(emotion,accuracy)\n",
    "plt.title = \"Sports Watch Data\"\n",
    "plt.xlabel = 'Month'\n",
    "plt.ylabel = \"Calorie Burnage\"\n",
    "plt.ylim(ymin=0)\n",
    "plt.ylim(ymax=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974d6a57-e869-4110-b522-8f0fd7df32c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
